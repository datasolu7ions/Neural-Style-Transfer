{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":15774,"status":"ok","timestamp":1720833048428,"user":{"displayName":"Beto García","userId":"09747885273711178654"},"user_tz":300},"id":"Bcn_P3b4oaWt"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torchvision.models import vgg19\n","from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":493,"status":"ok","timestamp":1720833048876,"user":{"displayName":"Beto García","userId":"09747885273711178654"},"user_tz":300},"id":"K6bZCAHUocSK"},"outputs":[],"source":["# Ruta a la imagen que desea transformar\n","target_image_path = '/content/drive/MyDrive/Transfer style con Pytorch/miFoto.jpg'\n","# Ruta de acceso a la imagen de estilo\n","style_reference_image_path = '/content/drive/MyDrive/Transfer style con Pytorch/styles/Van gogh.jpg'\n","# Dimensiones de la imagen generada\n","width, height = Image.open(target_image_path).size\n","img_height = 400\n","img_width = int(width * img_height / height)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18856,"status":"ok","timestamp":1720833067697,"user":{"displayName":"Beto García","userId":"09747885273711178654"},"user_tz":300},"id":"dS4aHAv0tNHV","outputId":"76b2cd52-d016-4b43-811f-ef28ff2a57f1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n","100%|██████████| 548M/548M [00:08<00:00, 68.0MB/s]\n"]}],"source":["# Preprocesamiento y desprocesamiento\n","def preprocess_image(image_path):\n","    transform = transforms.Compose([\n","        transforms.Resize((img_height, img_width)),\n","        transforms.ToTensor(),\n","        transforms.Lambda(lambda x: x.mul(255)),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","    image = Image.open(image_path)\n","    image = transform(image).unsqueeze(0)\n","    return image\n","\n","def deprocess_image(tensor):\n","    tensor = tensor.clone().detach().cpu()\n","    tensor = tensor.numpy().squeeze()\n","    tensor = tensor.transpose(1, 2, 0)\n","    tensor[:, :, 0] += 0.485 * 255\n","    tensor[:, :, 1] += 0.456 * 255\n","    tensor[:, :, 2] += 0.406 * 255\n","    tensor = tensor[:, :, ::-1]\n","    tensor = np.clip(tensor, 0, 255).astype('uint8')\n","    return tensor\n","\n","# Cargar imágenes\n","target_image = preprocess_image(target_image_path)\n","style_reference_image = preprocess_image(style_reference_image_path)\n","combination_image = target_image.clone().requires_grad_(True)\n","\n","# Modelo VGG19\n","cnn = vgg19(pretrained=True).features.eval()\n","\n","# Función de pérdida de contenido\n","def content_loss(base, combination):\n","    return torch.mean((combination - base) ** 2)\n","\n","# Función de pérdida de estilo\n","def gram_matrix(tensor):\n","    b, c, h, w = tensor.size()\n","    features = tensor.view(b * c, h * w)\n","    G = torch.mm(features, features.t())\n","    return G.div(b * c * h * w)\n","\n","def style_loss(style, combination):\n","    S = gram_matrix(style)\n","    C = gram_matrix(combination)\n","    return torch.mean((S - C) ** 2)\n","\n","# Función de pérdida de variación total\n","def total_variation_loss(x):\n","    b, c, h, w = x.size()\n","    a = torch.mean(torch.abs(x[:, :, :-1, :-1] - x[:, :, 1:, :-1]))\n","    b = torch.mean(torch.abs(x[:, :, :-1, :-1] - x[:, :, :-1, 1:]))\n","    return a + b\n","\n","# Capa utilizada para la pérdida de contenido\n","content_layer = '21'\n","# Capas utilizadas para la pérdida de estilo\n","style_layers = ['0', '5', '10', '19', '28']\n","# Ponderaciones en la media ponderada de los componentes de la pérdida\n","total_variation_weight = 1e-4\n","style_weight = 1.0\n","content_weight = 0.025\n","\n","# Extraer características\n","def get_features(image, model, layers=None):\n","    features = {}\n","    x = image\n","    for name, layer in model._modules.items():\n","        x = layer(x)\n","        if name in layers:\n","            features[name] = x\n","    return features\n","\n","target_features = get_features(target_image, cnn, layers={content_layer})\n","style_features = get_features(style_reference_image, cnn, layers=style_layers)\n","\n","# Optimizador\n","optimizer = optim.LBFGS([combination_image])\n","\n","# Función de actualización\n","def closure():\n","    combination_features = get_features(combination_image, cnn, layers={content_layer} | set(style_layers))\n","\n","    loss = content_weight * content_loss(target_features[content_layer], combination_features[content_layer])\n","    for layer in style_layers:\n","        loss += (style_weight / len(style_layers)) * style_loss(style_features[layer], combination_features[layer])\n","    loss += total_variation_weight * total_variation_loss(combination_image)\n","\n","    optimizer.zero_grad()\n","    loss.backward(retain_graph=True)\n","    return loss"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xPCcqus67fpW","executionInfo":{"status":"ok","timestamp":1720834587272,"user_tz":300,"elapsed":1519620,"user":{"displayName":"Beto García","userId":"09747885273711178654"}},"outputId":"6d44e23b-c6f9-41d1-b2f8-0cc6bf81b387"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1 completed\n","Iteration 2 completed\n","Iteration 3 completed\n","Iteration 4 completed\n","Iteration 5 completed\n"]}],"source":["# Iteraciones de optimización\n","iterations = 5\n","for i in range(iterations):\n","    optimizer.step(closure)\n","    print(f'Iteration {i+1} completed')\n","\n","# Guardar la imagen generada\n","final_img = deprocess_image(combination_image)\n","plt.imsave('result.png', final_img)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"mAYziB3n6Twg","executionInfo":{"status":"ok","timestamp":1720834592204,"user_tz":300,"elapsed":4956,"user":{"displayName":"Beto García","userId":"09747885273711178654"}}},"outputs":[],"source":["# Guardar Modelo\n","import torch\n","from torchvision.models import vgg19\n","import torch.optim as optim\n","\n","# Definir el modelo y el optimizador\n","cnn = vgg19(pretrained=True).features.eval()\n","optimizer = optim.LBFGS([combination_image])\n","\n","# Guardar el estado del modelo y el optimizador\n","checkpoint = {\n","    'model_state_dict': cnn.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    'target_image': target_image,\n","    'style_reference_image': style_reference_image,\n","    'combination_image': combination_image,\n","}\n","\n","torch.save(checkpoint, 'style_transfer_checkpoint.pth')\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HuDmfDO16ZzN","executionInfo":{"status":"ok","timestamp":1720834594026,"user_tz":300,"elapsed":1832,"user":{"displayName":"Beto García","userId":"09747885273711178654"}},"outputId":"612d1ba8-2baf-4218-bbbf-5e011625da78"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (1): ReLU(inplace=True)\n","  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (3): ReLU(inplace=True)\n","  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (6): ReLU(inplace=True)\n","  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (8): ReLU(inplace=True)\n","  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (11): ReLU(inplace=True)\n","  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (13): ReLU(inplace=True)\n","  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (15): ReLU(inplace=True)\n","  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (17): ReLU(inplace=True)\n","  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (20): ReLU(inplace=True)\n","  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (22): ReLU(inplace=True)\n","  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (24): ReLU(inplace=True)\n","  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (26): ReLU(inplace=True)\n","  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (29): ReLU(inplace=True)\n","  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (31): ReLU(inplace=True)\n","  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (33): ReLU(inplace=True)\n","  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (35): ReLU(inplace=True)\n","  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",")"]},"metadata":{},"execution_count":6}],"source":["#Cargar Modelo\n","import torch\n","from torchvision.models import vgg19\n","import torch.optim as optim\n","\n","# Cargar el punto de control\n","checkpoint = torch.load('style_transfer_checkpoint.pth')\n","\n","# Definir el modelo y el optimizador\n","cnn = vgg19(pretrained=True).features.eval()\n","optimizer = optim.LBFGS([checkpoint['combination_image']])\n","\n","# Restaurar los estados\n","cnn.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","# Restaurar las imágenes\n","target_image = checkpoint['target_image']\n","style_reference_image = checkpoint['style_reference_image']\n","combination_image = checkpoint['combination_image']\n","\n","# No olvides poner el modelo en modo de evaluación si solo vas a inferir\n","cnn.eval()\n"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1GwEyn0lFpintO0WyduzeSvb-UARw8t5l","authorship_tag":"ABX9TyMC58DlHaqW1aZLuBYp/NGO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}